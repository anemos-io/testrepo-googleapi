syntax = "proto3";

package google.cloud.bigquery.storage.v1beta2;

import "google/api/annotations.proto";
import "google/api/client.proto";
import "google/api/field_behavior.proto";
import "google/api/resource.proto";
import "google/cloud/bigquery/storage/v1beta2/arrow.proto";
import "google/cloud/bigquery/storage/v1beta2/avro.proto";
import "google/cloud/bigquery/storage/v1beta2/protobuf.proto";
import "google/cloud/bigquery/storage/v1beta2/stream.proto";
import "google/cloud/bigquery/storage/v1beta2/table.proto";
import "google/protobuf/timestamp.proto";
import "google/protobuf/wrappers.proto";
import "google/rpc/status.proto";

option java_package = "com.google.cloud.bigquery.storage.v1beta2";
option java_outer_classname = "StorageProto";
option java_multiple_files = true;
option go_package = "google.golang.org/genproto/googleapis/cloud/bigquery/storage/v1beta2;storage";


service BigQueryRead {
	option (google.api.default_host) = "bigquerystorage.googleapis.com";
	option (google.api.oauth_scopes) = "https://www.googleapis.com/auth/bigquery,https://www.googleapis.com/auth/bigquery.readonly,https://www.googleapis.com/auth/cloud-platform";

	rpc CreateReadSession(google.cloud.bigquery.storage.v1beta2.CreateReadSessionRequest) returns (google.cloud.bigquery.storage.v1beta2.ReadSession) {
		option (google.api.method_signature) = "parent,read_session,max_stream_count";
		option (google.api.http) = {
			post: "/v1beta2/{read_session.table=projects/*/datasets/*/tables/*}"
			body: "*"
		};

	}
	rpc ReadRows(google.cloud.bigquery.storage.v1beta2.ReadRowsRequest) returns (stream google.cloud.bigquery.storage.v1beta2.ReadRowsResponse) {
		option (google.api.method_signature) = "read_stream,offset";
		option (google.api.http) = {
			get: "/v1beta2/{read_stream=projects/*/locations/*/sessions/*/streams/*}"
		};

	}
	rpc SplitReadStream(google.cloud.bigquery.storage.v1beta2.SplitReadStreamRequest) returns (google.cloud.bigquery.storage.v1beta2.SplitReadStreamResponse) {
		option (google.api.http) = {
			get: "/v1beta2/{name=projects/*/locations/*/sessions/*/streams/*}"
		};

	}
}

service BigQueryWrite {
	option (google.api.default_host) = "bigquerystorage.googleapis.com";
	option (google.api.oauth_scopes) = "https://www.googleapis.com/auth/bigquery,https://www.googleapis.com/auth/bigquery.insertdata,https://www.googleapis.com/auth/cloud-platform";

	rpc CreateWriteStream(google.cloud.bigquery.storage.v1beta2.CreateWriteStreamRequest) returns (google.cloud.bigquery.storage.v1beta2.WriteStream) {
		option (google.api.method_signature) = "parent,write_stream";
		option (google.api.http) = {
			post: "/v1beta2/{parent=projects/*/datasets/*/tables/*}"
			body: "write_stream"
		};

	}
	rpc AppendRows(stream google.cloud.bigquery.storage.v1beta2.AppendRowsRequest) returns (stream google.cloud.bigquery.storage.v1beta2.AppendRowsResponse) {
		option (google.api.method_signature) = "write_stream";
		option (google.api.http) = {
			post: "/v1beta2/{write_stream=projects/*/datasets/*/tables/*/streams/*}"
			body: "*"
		};

	}
	rpc GetWriteStream(google.cloud.bigquery.storage.v1beta2.GetWriteStreamRequest) returns (google.cloud.bigquery.storage.v1beta2.WriteStream) {
		option (google.api.method_signature) = "name";
		option (google.api.http) = {
			post: "/v1beta2/{name=projects/*/datasets/*/tables/*/streams/*}"
			body: "*"
		};

	}
	rpc FinalizeWriteStream(google.cloud.bigquery.storage.v1beta2.FinalizeWriteStreamRequest) returns (google.cloud.bigquery.storage.v1beta2.FinalizeWriteStreamResponse) {
		option (google.api.method_signature) = "name";
		option (google.api.http) = {
			post: "/v1beta2/{name=projects/*/datasets/*/tables/*/streams/*}"
			body: "*"
		};

	}
	rpc BatchCommitWriteStreams(google.cloud.bigquery.storage.v1beta2.BatchCommitWriteStreamsRequest) returns (google.cloud.bigquery.storage.v1beta2.BatchCommitWriteStreamsResponse) {
		option (google.api.method_signature) = "parent";
		option (google.api.http) = {
			get: "/v1beta2/{parent=projects/*/datasets/*/tables/*}"
		};

	}
	rpc FlushRows(google.cloud.bigquery.storage.v1beta2.FlushRowsRequest) returns (google.cloud.bigquery.storage.v1beta2.FlushRowsResponse) {
		option (google.api.method_signature) = "write_stream";
		option (google.api.http) = {
			post: "/v1beta2/{write_stream=projects/*/datasets/*/tables/*/streams/*}"
			body: "*"
		};

	}
}

message CreateReadSessionRequest {

	string parent = 1 [
		(google.api.field_behavior) = REQUIRED
		(google.api.resource_reference) = {
			type: "cloudresourcemanager.googleapis.com/Project"
		}
	];
	google.cloud.bigquery.storage.v1beta2.ReadSession read_session = 2 [
		(google.api.field_behavior) = REQUIRED
	];
	int32 max_stream_count = 3;
}

message ReadRowsRequest {

	string read_stream = 1 [
		(google.api.field_behavior) = REQUIRED
		(google.api.resource_reference) = {
			type: "bigquerystorage.googleapis.com/ReadStream"
		}
	];
	int64 offset = 2;
}

message ThrottleState {

	int32 throttle_percent = 1;
}

message StreamStats {

	message Progress {

		double at_response_start = 1;
		double at_response_end = 2;
	}

	Progress progress = 2;
}

message ReadRowsResponse {

	oneof rows {
		google.cloud.bigquery.storage.v1beta2.AvroRows avro_rows = 3;
		google.cloud.bigquery.storage.v1beta2.ArrowRecordBatch arrow_record_batch = 4;
	}
	int64 row_count = 6;
	StreamStats stats = 2;
	ThrottleState throttle_state = 5;
	oneof schema {
		google.cloud.bigquery.storage.v1beta2.AvroSchema avro_schema = 7 [
			(google.api.field_behavior) = OUTPUT_ONLY
		];
		google.cloud.bigquery.storage.v1beta2.ArrowSchema arrow_schema = 8 [
			(google.api.field_behavior) = OUTPUT_ONLY
		];
	}
}

message SplitReadStreamRequest {

	string name = 1 [
		(google.api.field_behavior) = REQUIRED
		(google.api.resource_reference) = {
			type: "bigquerystorage.googleapis.com/ReadStream"
		}
	];
	double fraction = 2;
}

message SplitReadStreamResponse {

	google.cloud.bigquery.storage.v1beta2.ReadStream primary_stream = 1;
	google.cloud.bigquery.storage.v1beta2.ReadStream remainder_stream = 2;
}

message CreateWriteStreamRequest {

	string parent = 1 [
		(google.api.field_behavior) = REQUIRED
		(google.api.resource_reference) = {
			type: "bigquery.googleapis.com/Table"
		}
	];
	google.cloud.bigquery.storage.v1beta2.WriteStream write_stream = 2 [
		(google.api.field_behavior) = REQUIRED
	];
}

message AppendRowsRequest {

	message ProtoData {

		google.cloud.bigquery.storage.v1beta2.ProtoSchema writer_schema = 1;
		google.cloud.bigquery.storage.v1beta2.ProtoRows rows = 2;
	}

	string write_stream = 1 [
		(google.api.field_behavior) = REQUIRED
		(google.api.resource_reference) = {
			type: "bigquerystorage.googleapis.com/WriteStream"
		}
	];
	google.protobuf.Int64Value offset = 2;
	oneof rows {
		ProtoData proto_rows = 4;
	}
	string trace_id = 6;
}

message AppendRowsResponse {

	message AppendResult {

		google.protobuf.Int64Value offset = 1;
	}

	oneof response {
		AppendResult append_result = 1;
		google.rpc.Status error = 2;
	}
	google.cloud.bigquery.storage.v1beta2.TableSchema updated_schema = 3;
}

message GetWriteStreamRequest {

	string name = 1 [
		(google.api.field_behavior) = REQUIRED
		(google.api.resource_reference) = {
			type: "bigquerystorage.googleapis.com/WriteStream"
		}
	];
}

message BatchCommitWriteStreamsRequest {

	string parent = 1 [
		(google.api.field_behavior) = REQUIRED
	];
	repeated string write_streams = 2 [
		(google.api.field_behavior) = REQUIRED
	];
}

message BatchCommitWriteStreamsResponse {

	google.protobuf.Timestamp commit_time = 1;
	repeated StorageError stream_errors = 2;
}

message FinalizeWriteStreamRequest {

	string name = 1 [
		(google.api.field_behavior) = REQUIRED
		(google.api.resource_reference) = {
			type: "bigquerystorage.googleapis.com/WriteStream"
		}
	];
}

message FinalizeWriteStreamResponse {

	int64 row_count = 1;
}

message FlushRowsRequest {

	string write_stream = 1 [
		(google.api.field_behavior) = REQUIRED
		(google.api.resource_reference) = {
			type: "bigquerystorage.googleapis.com/WriteStream"
		}
	];
	google.protobuf.Int64Value offset = 2;
}

message FlushRowsResponse {

	int64 offset = 1;
}

message StorageError {

	enum StorageErrorCode {

		STORAGE_ERROR_CODE_UNSPECIFIED = 0;
		TABLE_NOT_FOUND = 1;
		STREAM_ALREADY_COMMITTED = 2;
		STREAM_NOT_FOUND = 3;
		INVALID_STREAM_TYPE = 4;
		INVALID_STREAM_STATE = 5;
		STREAM_FINALIZED = 6;
	}

	StorageErrorCode code = 1;
	string entity = 2;
	string error_message = 3;
}
